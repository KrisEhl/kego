{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "import polars as pl\n",
    "import sklearn\n",
    "\n",
    "import kego.plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COMPETITION = pathlib.Path(\"../../data/playground/playground-series-s5e9\")\n",
    "PATH_TRAIN = PATH_COMPETITION / \"train.csv\"\n",
    "PATH_TEST = PATH_COMPETITION / \"test.csv\"\n",
    "os.listdir(PATH_COMPETITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(PATH_TRAIN)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kego.plotting.plot_histogram(\"BeatsPerMinute\", df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for all features using Polars\n",
    "correlation_matrix = train.select(pl.all().exclude(\"id\")).corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(np.array(correlation_matrix), cmap=\"bwr\", vmin=-0.1, vmax=0.1)\n",
    "plt.xticks(\n",
    "    range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90\n",
    ")\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "fig.colorbar(cax)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"BeatsPerMinute\"\n",
    "FEATURES = [col for col in train.columns if col not in (\"id\", TARGET)]\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way to split Polars DataFrame into train, validation, and test sets\n",
    "train_with_rand = train.with_columns(pl.col(\"id\").shuffle(seed=42).alias(\"_rand\"))\n",
    "n = train_with_rand.height\n",
    "train_split = train_with_rand.filter(pl.col(\"_rand\") < int(0.7 * n)).drop(\"_rand\")\n",
    "validate_split = train_with_rand.filter(\n",
    "    (pl.col(\"_rand\") >= int(0.7 * n)) & (pl.col(\"_rand\") < int(0.85 * n))\n",
    ").drop(\"_rand\")\n",
    "test_split = train_with_rand.filter(pl.col(\"_rand\") >= int(0.85 * n)).drop(\"_rand\")\n",
    "train_split.shape, validate_split.shape, test_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validate_split, features=FEATURES, target=TARGET):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    X_validate = validate_split[features].to_numpy()\n",
    "    y_validate = validate_split[target].to_numpy()\n",
    "\n",
    "    y_pred = model.predict(X_validate)\n",
    "\n",
    "    mse = mean_squared_error(y_validate, y_pred)\n",
    "    r2 = r2_score(y_validate, y_pred)\n",
    "\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train histogramgradientboost regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "# model.fit(train_split.select(FEATURES).to_numpy(), train_split[TARGET].to_numpy())\n",
    "# sklearn.metrics.mean_squared_error(model.predict(train_split[FEATURES].to_numpy()), train_split[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with HalvingGridSearchCV for GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "}\n",
    "base_model = GradientBoostingRegressor()\n",
    "halving_cv = HalvingGridSearchCV(\n",
    "    base_model, param_grid, factor=2, random_state=42, n_jobs=-1, verbose=1\n",
    ")\n",
    "# halving_cv.fit(\n",
    "#     train_split.select(FEATURES).to_numpy(), train_split[TARGET].to_numpy()\n",
    "# )\n",
    "# print(\"Best parameters:\", halving_cv.best_params_)\n",
    "# print(\"Best score:\", halving_cv.best_score_)\n",
    "# validate_model(halving_cv.best_estimator_, validate_split)\n",
    "# validate_model(halving_cv.best_estimator_, train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.DataFrame(halving_cv.cv_results_).select(\n",
    "#     [\n",
    "#         \"param_n_estimators\",\n",
    "#         \"param_max_depth\",\n",
    "#         \"param_learning_rate\",\n",
    "#         \"mean_test_score\",\n",
    "#         \"rank_test_score\",\n",
    "#     ]\n",
    "# ).sort(\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try RandomForestRegressor for comparison\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=42, n_jobs=-1)\n",
    "# rf_model.fit(train_split.select(FEATURES).to_numpy(), train_split[TARGET].to_numpy())\n",
    "# rf_mse, rf_r2 = validate_model(rf_model, validate_split)\n",
    "# print(f\"RandomForestRegressor MSE: {rf_mse:.2f}, R2: {rf_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature scaling with StandardScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(train_split.select(FEATURES).to_numpy())\n",
    "# X_validate_scaled = scaler.transform(validate_split.select(FEATURES).to_numpy())\n",
    "# y_train = train_split[TARGET].to_numpy()\n",
    "# y_validate = validate_split[TARGET].to_numpy()\n",
    "# model_scaled = GradientBoostingRegressor()\n",
    "# model_scaled.fit(X_train_scaled, y_train)\n",
    "# mse_scaled, r2_scaled = validate_model(model_scaled, validate_split)\n",
    "# print(f\"Scaled GradientBoostingRegressor MSE: {mse_scaled:.2f}, R2: {r2_scaled:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and evaluate XGBoost regressor\n",
    "# import xgboost as xgb\n",
    "# xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=7, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "# xgb_model.fit(train_split.select(FEATURES).to_numpy(), train_split[TARGET].to_numpy())\n",
    "# xgb_mse, xgb_r2 = validate_model(xgb_model, validate_split)\n",
    "# print(f\"XGBoostRegressor MSE: {xgb_mse:.2f}, R2: {xgb_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions and relationships\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_sample = train.sample(n=min(1000, train.height))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_sample[\"BeatsPerMinute\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of BeatsPerMinute\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.pairplot(df_sample.to_pandas(), vars=FEATURES + [TARGET])\n",
    "plt.suptitle(\"Pairplot of Features and Target\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks: missing values, duplicates, outliers\n",
    "print(\"Missing values per column:\")\n",
    "print(train.null_count())\n",
    "print(\"\\nNumber of duplicate rows:\")\n",
    "print(train.is_duplicated().sum())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(train.describe())\n",
    "# Rescale features to max value of 1 for boxplot\n",
    "train_rescaled = train.with_columns(\n",
    "    [pl.col(col) / pl.col(col).max() for col in FEATURES + [TARGET]]\n",
    ")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train_rescaled.to_pandas()[FEATURES + [TARGET]])\n",
    "plt.title(\"Boxplot of Features and Target (Rescaled)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using IQR method for all features and target\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        df = df.filter((pl.col(col) >= lower) & (pl.col(col) <= upper))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_no_outliers = remove_outliers(train, FEATURES + [TARGET])\n",
    "print(\n",
    "    f\"Original shape: {train.shape}, After outlier removal: {train_no_outliers.shape}\"\n",
    ")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train_no_outliers.to_pandas()[FEATURES + [TARGET]])\n",
    "plt.title(\"Boxplot After Outlier Removal\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_gradient_boost(df):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(df.select(FEATURES).to_numpy(), df[TARGET].to_numpy())\n",
    "    return model, sklearn.metrics.mean_squared_error(\n",
    "        model.predict(df[FEATURES].to_numpy()), df[TARGET]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_gradient_boost(train_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_gradient_boost(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: add polynomial features and interactions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(train_no_outliers.select(FEATURES).to_numpy())\n",
    "print(\n",
    "    f\"Original feature count: {len(FEATURES)}, After polynomial expansion: {X_poly.shape[1]}\"\n",
    ")\n",
    "model_poly = GradientBoostingRegressor()\n",
    "model_poly.fit(X_poly, train_no_outliers[TARGET].to_numpy())\n",
    "print(\"Trained GradientBoostingRegressor with polynomial features\")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred_poly = model_poly.predict(X_poly)\n",
    "mse_poly = mean_squared_error(train_no_outliers[TARGET].to_numpy(), y_pred_poly)\n",
    "r2_poly = r2_score(train_no_outliers[TARGET].to_numpy(), y_pred_poly)\n",
    "print(\n",
    "    f\"GradientBoostingRegressor with polynomial features MSE: {mse_poly:.2f}, R2: {r2_poly:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-series-s5e9 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
